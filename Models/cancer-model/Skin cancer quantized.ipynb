{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import the libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q tensorflow-model-optimization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom glob import glob\nimport seaborn as sns\nimport pprint as pp\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import resample\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_model_optimization as tfmot\nimport keras.utils\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\n\nimport itertools\n\nimport cv2\nfrom PIL import Image\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"base_skin_dir = os.path.join('..', 'input/skin-cancer-mnist-ham10000')\n\n# Merging images from both folders HAM10000_images_part1.zip and HAM10000_images_part2.zip into one dictionary\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n                     for x in glob(os.path.join(base_skin_dir, '*', '*.jpg'))}\n\n# This dictionary is useful for displaying more human-friendly labels later on\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'Melanoma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))\n\n# Creating New Columns for better readability\ndata['path'] = data['image_id'].map(imageid_path_dict.get)\ndata['cell_type'] = data['dx'].map(lesion_type_dict.get) \ndata['cell_type_idx'] = pd.Categorical(data['cell_type']).codes\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(1, 1, figsize = (10, 5))\ndata['cell_type'].value_counts().plot(kind='bar', ax=ax1, color='teal', alpha=0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the dataset is highly imbalanced. The number of images for Melanocytic Levi is much more than those for the other classes. So let's resample/upsample each class to 7000 instances. The code below returns a balanced dataset, in which each class has 7000 instances. Of course this leads to heavily redundant data in categories with originally very few instances."},{"metadata":{"trusted":true},"cell_type":"code","source":"def balanced_dataset(df):\n    df_balanced = pd.DataFrame()\n    #df = pd.DataFrame()\n    \n    for cat in df['cell_type_idx'].unique():\n        temp = resample(df[df['cell_type_idx'] == cat], \n                        replace=True,     # sample with replacement\n                        n_samples=7000,   # to match majority class\n                        random_state=123) # reproducible results\n\n        # Combine majority class with upsampled minority class\n        df_balanced = pd.concat([df_balanced, temp])\n \n    df_balanced['cell_type'].value_counts()\n\n    return df_balanced","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rescaling the images to (128,128,3) and standardizing (division by 255)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef load_img_data(size, df, balanced=False):\n    #first we should normalize the image from 0-255 to 0-1\n    \n    img_h, img_w = size, size\n    imgs = []\n    \n    if balanced:\n        df = balanced_dataset(df)\n    \n    image_paths = list(df['path'])\n\n    for i in tqdm(range(len(image_paths))):\n        img = cv2.imread(image_paths[i])\n        img = cv2.resize(img, (img_h, img_w))\n        img = img.astype(np.float32) / 255.\n        #img = np.asarray(Image.open(image_paths[i]).resize((size,size)))\n        imgs.append(img)\n\n    imgs = np.stack(imgs, axis=0)\n    print(imgs.shape)\n    #imgs = imgs.astype(np.float32) / 255.\n    \n    return imgs, df['cell_type_idx'].values\n\nimgs, target   = load_img_data(128, data, balanced=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building The model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(imgs, target, test_size=0.20)\nx_train, x_val, y_train, y_val = train_test_split(imgs, target, test_size=0.05)\n\ntrain_val_test = (x_train, y_train, x_val, y_val, x_test, y_test)\n\n[x_train.shape, x_val.shape, x_test.shape]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the various different architectures mentioned Densenet201 worked best.\nThe training is done in 2 phases:\n\n* Phase 1: \n           Only the new dense layers added on top of the base model is trained and the base model's layers\n           remain frozen. This makes the output of the CNN convolutions remain stable and allows the dense\n           layers to learn to classify the extracted features to classes.\n          \n* Phase 2: \n           Here we additionally fine tune the the entire model to further increase the predictive accuracy\n           of the network. A lower learning rate is used to prevent too drastic changes to the feature extracters.\n         \nIf the full CNN would be trained immediately, i.e. skipping phase 1, the completely untrained dense\nlayers would initially create close-to-random predictions leading to a high loss. This loss would\nthen be back-propagated through the whole CNN and likely “break” the already well-trained feature detectors.\n\n         \n* In phase 2 we can also train selected layers of the base model but training the entire model resulted better. \n* Data augmentation did not result in better results probably because of already upsampled data. So there was no\n  need, and in fact this resulted in better acc.    \n* Custom weights for different classes was tried but resulted in weaker training progress so it has been\n  commented out."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Tuner(object):\n\n    def __init__(self, data, architecture, hidden_layers, classes, epochs, batch_size):\n        self.input_shape = data[0][0].shape\n\n        self.base_arch = architecture\n        self.nn = self.download_network()\n        self.nn.trainable = False\n\n        self.hidden_layers = hidden_layers\n        \n        self.classes = classes\n\n        self.trainX = data[0]\n        self.trainY = data[1]\n        self.valX = data[2]\n        self.valY = data[3]\n        self.testX = data[4]\n        self.testY = data[5]\n\n        self.EPOCHS = epochs\n        self.BATCH_SIZE = batch_size\n        \n        self.model = self.build()\n        self.train_generator = self.data_augmentation()\n        self.predictions = None\n        self.score = None\n\n        self.best_weights = None\n        \n    def download_network(self):\n        #Download the requested CNN with imagenet weights\n\n        nn = None\n\n        if self.base_arch == 'VGG16':\n            nn = tf.keras.applications.VGG16(weights='imagenet', include_top=False, \n                                             input_shape=self.input_shape)\n        elif self.base_arch == 'VGG19':\n            nn = tf.keras.applications.VGG19(weights='imagenet', include_top=False, \n                                             input_shape=self.input_shape)\n        elif self.base_arch == 'InceptionV3':\n            nn = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'Xception':\n            nn = tf.keras.applications.Xception(weights='imagenet', include_top=False, \n                                                input_shape=self.input_shape)\n        elif self.base_arch == 'DenseNet121':\n            nn = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'DenseNet201':\n            nn = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'ResNet152V2':\n            nn = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'MobileNet':\n            nn = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, \n                                                 input_shape=self.input_shape)\n        elif self.base_arch == 'MobileNetV2':\n            nn = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'EfficientNetB5':\n            nn = tf.keras.applications.EfficientNetB5(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n\n        return nn\n    \n    def run(self):\n        '''\n        Main driver for Learner object\n        '''\n        self.fine_tune()\n        #self.load_weights(self.best_weights)\n        #self.predict()\n        \n    def build(self):\n        '''\n        Build model. Add Dense layer to topless base CNN.\n        '''\n\n        model = tf.keras.models.Sequential()\n        model.add(self.nn)\n        model.add(tf.keras.layers.Flatten())\n        model.add(tf.keras.layers.Dropout(0.25))\n        \n        for layer in self.hidden_layers:\n            model.add(tf.keras.layers.Dense(layer, activation='relu'))\n            model.add(tf.keras.layers.BatchNormalization())\n            model.add(tf.keras.layers.Dropout(0.46))  \n\n        model.add(tf.keras.layers.Dense(self.classes, activation='softmax'))\n        \n        print (model.summary())\n\n        return model\n    \n    def load_weights(self, name):\n        #Load the best checkpointed weights.\n        \n        print('\\nLoading best accuracy weights.')\n        self.model.load_weights(name)\n        \n    def data_augmentation(self):\n        data_gen_args = dict(\n                rotation_range=10,\n                zoom_range=0.1,\n                shear_range=0.1,\n                width_shift_range=0.1, \n                height_shift_range=0.1,\n                horizontal_flip=True,\n                vertical_flip=True,\n            )\n\n        train_gen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)\n        train_generator = train_gen.flow(self.trainX, self.trainY, batch_size=self.BATCH_SIZE)\n\n        #print('\\nData augmentation with the following parameters:')\n        #pp.pprint(data_gen_args)\n\n        return train_generator\n    \n    \n    def fine_tune(self):\n        '''\n        Fine-tune network in 2 phases\n        '''\n\n        numTrainingSamples = self.trainX.shape[0]\n        numValidationSamples = self.valX.shape[0]\n\n        print (\"\\nPhase A - Training Fully Connected Layers\\n\")\n        self.model.compile(loss='sparse_categorical_crossentropy', \n                           optimizer=tf.keras.optimizers.Adam(lr=0.001), \n                           metrics=['accuracy'])\n\n        # Define checkpoint to save best Phase 1 weights\n        best_weights_ph1 = self.base_arch + \"_ph1_weights.hdf5\"\n        checkpoint = [tfmot.sparsity.keras.UpdatePruningStep(), \n                      tf.keras.callbacks.ModelCheckpoint(best_weights_ph1, monitor=\"val_loss\", \n                                                         mode=\"min\", save_best_only=True, verbose=1)]\n\n        \n        history = self.model.fit(\n            x_train, y_train,\n            #self.train_generator,\n            steps_per_epoch=numTrainingSamples // self.BATCH_SIZE,\n            epochs=self.EPOCHS,\n            validation_data=(self.valX, self.valY),\n            validation_steps=numValidationSamples // self.BATCH_SIZE,\n            #class_weight=self.get_class_weight(),\n            callbacks=checkpoint)\n        \n        # Store the best phase 1 accuracy\n        best_acc_ph1 = max(history.history[\"val_accuracy\"])\n        print('\\n\\nMax validation accuracy:', best_acc_ph1)\n\n        print('\\nRestoring best weights and predicting validation set.')\n        self.load_weights(best_weights_ph1)\n\n        # Make predictions based on best phase 1 weights\n        self.predict()\n\n        self.plot_loss(history, self.EPOCHS, '\\n Transfer Learning: ' + self.base_arch + ' Ph A')\n        \n        print (\"\\nPhase B  - Fine Tune all Layers \\n\")\n        # Set full original CNN as trainable\n        self.nn.trainable = True\n\n        self.model.compile(loss='sparse_categorical_crossentropy', \n                           optimizer=tf.keras.optimizers.Adam(lr=1e-5), \n                           metrics=['accuracy'])\n        \n        # Define checkpoint to save best Phase 2 weights\n        best_weights_ph2 = self.base_arch + \"_ph2_weights.hdf5\"\n        checkpoint = [tfmot.sparsity.keras.UpdatePruningStep(), \n                      tf.keras.callbacks.ModelCheckpoint(best_weights_ph2, monitor=\"val_loss\", mode=\"min\", \n                                                         save_best_only=True, verbose=1)]\n\n\n        # Fine-tune the full CNN + FC\n        history = self.model.fit(\n            x_train, y_train,\n            #self.train_generator,\n            steps_per_epoch=numTrainingSamples // self.BATCH_SIZE,\n            epochs=self.EPOCHS,\n            validation_data=(self.valX, self.valY),\n            validation_steps=numValidationSamples // self.BATCH_SIZE,\n            #class_weight=self.get_class_weight(),\n            callbacks=checkpoint)\n        \n        # Store the best phase 2 accuracy\n        best_acc_ph2 = max(history.history[\"val_accuracy\"])\n        print('\\n\\nMax validation accuracy:', best_acc_ph2)\n\n        # Only if Phase 2 fine-tuning resulted in a better accuracy than phase 1,\n        # restore best phase 2 weights and update Tuner predictions.\n        if best_acc_ph2 > best_acc_ph1:\n            print('\\nPhase 2 resulted in better accuracy than Phase 1.')\n            print('Restoring best weights of Ph2 and predicting validation set.')\n            self.load_weights(best_weights_ph2)\n            self.predict()\n\n        self.plot_loss(history, self.EPOCHS, '\\n Transfer Learning: ' + self.base_arch + ' Ph B')\n        \n    def predict(self):\n        '''\n        Get predictions and score for validation set.\n        '''\n        print('\\nPredicting test set classes.')\n        self.score = self.model.evaluate(self.testX, self.testY, verbose=0)\n        print('Test set score:', self.score)\n        self.predictions = self.model.predict(self.testX, batch_size=self.BATCH_SIZE)\n        print('Done')\n\n    def plot_loss(self, history, epochs, name):\n        print('\\n\\n')\n        plt.figure(figsize=(12,8))\n        plt.plot(np.arange(0, epochs), history.history[\"loss\"], label=\"train_loss\")\n        plt.plot(np.arange(0, epochs), history.history[\"val_loss\"], label=\"val_loss\")\n        plt.plot(np.arange(0, epochs), history.history[\"accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, epochs), history.history[\"val_accuracy\"], label=\"val_acc\")\n        plt.title(\"Training Loss and Accuracy - {}\".format(name))\n        plt.xlabel(\"Epoch #\")\n        plt.ylabel(\"Loss/Accuracy\")\n        plt.legend()\n        plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nNET = 'DenseNet201'\nHIDDEN_LAYERS = [512, 128]\n#HIDDEN_LAYERS = [256, 256, 64]\nCLASSES = len(set(target))\nBATCH_SIZE = 128\nEPOCHS = 20\n\ncrmodel = Tuner(train_val_test, NET, HIDDEN_LAYERS, CLASSES, EPOCHS, BATCH_SIZE)\ncrmodel.run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crmodel.predict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to plot confusion matrix    \ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    # Predict the values from the validation dataset\nY_pred = crmodel.predictions\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\n#Y_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_test, Y_pred_classes)\n\n \n\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pruning, Quantizing and Converting to Tflite model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_for_export = tfmot.sparsity.keras.strip_pruning(crmodel.model)\nconverter =tf.lite.TFLiteConverter.from_keras_model(model_for_export)\npruned_tflite_model = converter.convert()\nopen(\"pruned_tflite_model.tflite\", \"wb\").write(pruned_tflite_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"converter1 =tf.lite.TFLiteConverter.from_keras_model(model_for_export)\nconverter1.optimizations = [tf.lite.Optimize.DEFAULT]\nquant_tflite_model = converter1.convert()\nopen(\"quant_tflite_model.tflite\", \"wb\").write(quant_tflite_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the tflite model"},{"metadata":{"trusted":true},"cell_type":"code","source":"pruned_tflite_model_file = 'pruned_tflite_model.tflite'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pruned_interpreter = tf.lite.Interpreter(model_path=str(pruned_tflite_model_file))\npruned_interpreter.allocate_tensors()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quant_tflite_model_file='quant_tflite_model.tflite'\nquant_interpreter = tf.lite.Interpreter(model_path=str(quant_tflite_model_file))\nquant_interpreter.allocate_tensors()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is not the metric that is used for actual accuracy estimation.\nThis is done just to check if quantization of the tflite model reduces accuracy.\nIf yes, then how much?\nIf no, then gg."},{"metadata":{"trusted":true},"cell_type":"code","source":"# A helper function to evaluate the TF Lite model using \"test\" dataset.\ndef evaluate_model(interpreter):\n  input_index = interpreter.get_input_details()[0][\"index\"]\n  output_index = interpreter.get_output_details()[0][\"index\"]\n\n  # Run predictions on every image in the \"test\" dataset.\n  prediction_digits = []\n  for test_image in x_test:\n    # Pre-processing: add batch dimension and convert to float32 to match with\n    # the model's input data format.\n    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n    interpreter.set_tensor(input_index, test_image)\n\n    # Run inference.\n    interpreter.invoke()\n\n    # Post-processing: remove batch dimension and find the digit with highest\n    # probability.\n    output = interpreter.tensor(output_index)\n    digit = np.argmax(output()[0])\n    prediction_digits.append(digit)\n\n  # Compare prediction results with ground truth labels to calculate accuracy.\n  accurate_count = 0\n  for index in range(len(prediction_digits)):\n    if prediction_digits[index] == y_test[index]:\n      accurate_count += 1\n  accuracy = accurate_count * 1.0 / len(prediction_digits)\n\n  return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluate_model(pruned_interpreter))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluate_model(quant_interpreter))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}